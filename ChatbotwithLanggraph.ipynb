{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BjEufZwXF2kH",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0996b41d-a75a-4e97-eeec-edec4422f439"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langgraph\n",
            "  Downloading langgraph-0.2.28-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting langchain-core<0.4,>=0.2.39 (from langgraph)\n",
            "  Downloading langchain_core-0.3.6-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting langgraph-checkpoint<2.0.0,>=1.0.2 (from langgraph)\n",
            "  Downloading langgraph_checkpoint-1.0.11-py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.2.39->langgraph) (6.0.2)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.4,>=0.2.39->langgraph)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.125 (from langchain-core<0.4,>=0.2.39->langgraph)\n",
            "  Downloading langsmith-0.1.129-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.2.39->langgraph) (24.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.2.39->langgraph) (2.9.2)\n",
            "Collecting tenacity!=8.4.0,<9.0.0,>=8.1.0 (from langchain-core<0.4,>=0.2.39->langgraph)\n",
            "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.2.39->langgraph) (4.12.2)\n",
            "Collecting msgpack<2.0.0,>=1.1.0 (from langgraph-checkpoint<2.0.0,>=1.0.2->langgraph)\n",
            "  Downloading msgpack-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.2.39->langgraph)\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting httpx<1,>=0.23.0 (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.2.39->langgraph)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.2.39->langgraph)\n",
            "  Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.2.39->langgraph) (2.32.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.2.39->langgraph) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.2.39->langgraph) (2.23.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.2.39->langgraph) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.2.39->langgraph) (2024.8.30)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.2.39->langgraph)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.2.39->langgraph) (3.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.2.39->langgraph) (1.3.1)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.2.39->langgraph)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.2.39->langgraph) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.2.39->langgraph) (2.2.3)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.2.39->langgraph) (1.2.2)\n",
            "Downloading langgraph-0.2.28-py3-none-any.whl (107 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.7/107.7 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.3.6-py3-none-any.whl (399 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m399.9/399.9 kB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_checkpoint-1.0.11-py3-none-any.whl (17 kB)\n",
            "Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Downloading langsmith-0.1.129-py3-none-any.whl (292 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m292.2/292.2 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading msgpack-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (378 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m378.0/378.0 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
            "Downloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tenacity, orjson, msgpack, jsonpointer, h11, jsonpatch, httpcore, httpx, langsmith, langchain-core, langgraph-checkpoint, langgraph\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 9.0.0\n",
            "    Uninstalling tenacity-9.0.0:\n",
            "      Successfully uninstalled tenacity-9.0.0\n",
            "  Attempting uninstall: msgpack\n",
            "    Found existing installation: msgpack 1.0.8\n",
            "    Uninstalling msgpack-1.0.8:\n",
            "      Successfully uninstalled msgpack-1.0.8\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.2 jsonpatch-1.33 jsonpointer-3.0.0 langchain-core-0.3.6 langgraph-0.2.28 langgraph-checkpoint-1.0.11 langsmith-0.1.129 msgpack-1.1.0 orjson-3.10.7 tenacity-8.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install langgraph"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langsmith"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "gxfvtFrBHtqo",
        "outputId": "17402686-6840-42e5-dad3-dff56ad80e83"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langsmith in /usr/local/lib/python3.10/dist-packages (0.1.129)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith) (0.27.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith) (3.10.7)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langsmith) (2.9.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langsmith) (2.32.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith) (1.0.5)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith) (3.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langsmith) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langsmith) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langsmith) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith) (2.2.3)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith) (1.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain_groq langchain_community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "09OXqWpMHttU",
        "outputId": "1e9b5dbc-34c7-4446-883a-b9365af98cd9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.3.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting langchain_groq\n",
            "  Downloading langchain_groq-0.2.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.3.1-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.35)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.10.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.6 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.6)\n",
            "Collecting langchain-text-splitters<0.4.0,>=0.3.0 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.3.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.129)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.9.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.5.0)\n",
            "Collecting groq<1,>=0.4.1 (from langchain_groq)\n",
            "  Downloading groq-0.11.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n",
            "  Downloading pydantic_settings-2.5.2-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.11.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading marshmallow-3.22.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain_groq) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from groq<1,>=0.4.1->langchain_groq) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain_groq) (0.27.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain_groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain_groq) (4.12.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.6->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.6->langchain) (24.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.7)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain_community)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.8.30)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq<1,>=0.4.1->langchain_groq) (1.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain_groq) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain_groq) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.6->langchain) (3.0.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Downloading langchain-0.3.1-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m49.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_groq-0.2.0-py3-none-any.whl (14 kB)\n",
            "Downloading langchain_community-0.3.1-py3-none-any.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m86.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading groq-0.11.0-py3-none-any.whl (106 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.5/106.5 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_text_splitters-0.3.0-py3-none-any.whl (25 kB)\n",
            "Downloading pydantic_settings-2.5.2-py3-none-any.whl (26 kB)\n",
            "Downloading marshmallow-3.22.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, typing-inspect, pydantic-settings, groq, dataclasses-json, langchain-text-splitters, langchain_groq, langchain, langchain_community\n",
            "Successfully installed dataclasses-json-0.6.7 groq-0.11.0 langchain-0.3.1 langchain-text-splitters-0.3.0 langchain_community-0.3.1 langchain_groq-0.2.0 marshmallow-3.22.0 mypy-extensions-1.0.0 pydantic-settings-2.5.2 python-dotenv-1.0.1 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "groq_api_key = userdata.get('groq_api_key')\n",
        "langsmith = userdata.get('LANGSMITH_API_KEY')"
      ],
      "metadata": {
        "id": "0JwB87w2Htvu"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = langsmith\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"CourseLanggraph\""
      ],
      "metadata": {
        "id": "9kQXzz-UHtyN"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_groq import ChatGroq"
      ],
      "metadata": {
        "id": "2H-SFV2DHt0g"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatGroq(groq_api_key = groq_api_key, model_name =\"gemma2-9b-it\")\n",
        "llm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "laz0yVd7Ht3M",
        "outputId": "fd411ae5-144c-4d6f-cd42-8125c591f865"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x7e8b0798f970>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x7e8b07a542e0>, model_name='gemma2-9b-it', model_kwargs={}, groq_api_key=SecretStr('**********'))"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Now We will start making our Chatbot\n",
        "from typing import Annotated\n",
        "from typing_extensions import TypedDict\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.graph.message import add_messages"
      ],
      "metadata": {
        "id": "T-sORUeyOVXI"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class State(TypedDict):\n",
        "  messages:Annotated[list, add_messages]\n",
        "\n",
        "graph_builder = StateGraph(State)\n",
        "graph_builder"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_fznEj4OVZg",
        "outputId": "537f5146-f102-4461-88ea-45b81962f1ae"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x7e8b0798e620>"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def chatbot(state: State):\n",
        "  return {\"messages\": llm.invoke(state['messages'])}"
      ],
      "metadata": {
        "id": "GoUTf70SOVbw"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "graph_builder.add_node(\"chatbot\", chatbot)\n",
        "graph_builder"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJd_iUa3OVeI",
        "outputId": "bca49967-72d8-489c-a608-d68a320fe4f4"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x7e8b0798e620>"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "graph_builder.add_edge(START, \"chatbot\")\n",
        "graph_builder.add_edge(\"chatbot\", END)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DaAw6wegOVgE",
        "outputId": "c6a87e42-2d90-4b98-ecd3-f2679a8eac66"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x7e8b0798e620>"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "graph = graph_builder.compile()"
      ],
      "metadata": {
        "id": "gfLUTyEwSvxO"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image, display\n",
        "try:\n",
        "  display(Image(graph.get_graph().draw_mermaid_png()))\n",
        "except Exception:\n",
        "  pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "id": "ILNJiGbDSvze",
        "outputId": "7077e55b-aa2b-48bd-f346-aaee28dcd56b"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/jpeg": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "  user_input = input(\"User: \")\n",
        "  if user_input.lower() in [\"quit\", \"q\"]:\n",
        "    print(\"Good Bye\")\n",
        "    break\n",
        "  for event in graph.stream({'messages': (\"user\", user_input)}):\n",
        "    print(event.values())\n",
        "    for value in event.values():\n",
        "      print(value['messages'])\n",
        "      print(\"Assistant:\", value[\"messages\"].content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AizQnyQKTdem",
        "outputId": "01d9365d-b889-4b9b-8aaf-4e415cb0bab7"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User: hello\n",
            "dict_values([{'messages': AIMessage(content='Hello! 👋  How can I help you today? 😊\\n', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 10, 'total_tokens': 25, 'completion_time': 0.027272727, 'prompt_time': 4.3e-07, 'queue_time': 0.013767959000000001, 'total_time': 0.027273157}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-88313149-a2d9-4a5a-8d8f-8f8d86fa33bc-0', usage_metadata={'input_tokens': 10, 'output_tokens': 15, 'total_tokens': 25})}])\n",
            "content='Hello! 👋  How can I help you today? 😊\\n' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 10, 'total_tokens': 25, 'completion_time': 0.027272727, 'prompt_time': 4.3e-07, 'queue_time': 0.013767959000000001, 'total_time': 0.027273157}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None} id='run-88313149-a2d9-4a5a-8d8f-8f8d86fa33bc-0' usage_metadata={'input_tokens': 10, 'output_tokens': 15, 'total_tokens': 25}\n",
            "Assistant: Hello! 👋  How can I help you today? 😊\n",
            "\n",
            "User: what is the date today ?\n",
            "dict_values([{'messages': AIMessage(content='I can tell you the date!  Today is **October 26, 2023**. \\n', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 15, 'total_tokens': 41, 'completion_time': 0.047272727, 'prompt_time': 9.6509e-05, 'queue_time': 0.013479531, 'total_time': 0.047369236}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-06444915-5b44-452d-ad25-d96fcf98af64-0', usage_metadata={'input_tokens': 15, 'output_tokens': 26, 'total_tokens': 41})}])\n",
            "content='I can tell you the date!  Today is **October 26, 2023**. \\n' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 15, 'total_tokens': 41, 'completion_time': 0.047272727, 'prompt_time': 9.6509e-05, 'queue_time': 0.013479531, 'total_time': 0.047369236}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None} id='run-06444915-5b44-452d-ad25-d96fcf98af64-0' usage_metadata={'input_tokens': 15, 'output_tokens': 26, 'total_tokens': 41}\n",
            "Assistant: I can tell you the date!  Today is **October 26, 2023**. \n",
            "\n",
            "User: how to fine tune a neural network ?\n",
            "dict_values([{'messages': AIMessage(content=\"Let's break down how to fine-tune a pre-trained neural network.\\n\\n**Why Fine-Tune?**\\n\\nFine-tuning is like taking a brilliant student (your pre-trained model) who has a strong foundation in general knowledge and specializing them in your specific subject.\\n\\n* **Efficiency:**  Training a neural network from scratch is computationally expensive and time-consuming. Fine-tuning leverages the knowledge already learned by a pre-trained model, significantly reducing training time and resources.\\n* **Improved Performance:** Fine-tuning often leads to better performance on your target task compared to training a model from scratch, especially if your dataset is relatively small.\\n\\n**Steps to Fine-Tune a Neural Network**\\n\\n1. **Choose a Pre-trained Model:**\\n\\n   * Select a model architecture that's relevant to your task.  Popular choices include:\\n     * **Image Classification:** ResNet, Inception, VGG\\n     * **Natural Language Processing (NLP):** BERT, GPT, RoBERTa\\n     * **Object Detection:** YOLO, Faster R-CNN\\n   * Consider the size and complexity of the model. Larger models have more parameters and require more resources.\\n\\n2. **Prepare Your Dataset:**\\n\\n   * **Clean and Format:** Ensure your data is clean, well-formatted, and in a structure compatible with your chosen model.\\n   * **Split:** Divide your dataset into training, validation, and test sets. The training set is used to fine-tune the model, the validation set helps monitor performance during training, and the test set is used for a final evaluation of the model's ability to generalize to unseen data.\\n\\n3. **Load the Pre-trained Model:**\\n\\n   * Use a deep learning library like TensorFlow or PyTorch to load the pre-trained weights into your chosen model architecture.\\n\\n4. **Adjust the Model (if needed):**\\n\\n   * **Add/Remove Layers:** You might need to add or remove layers to match your specific task. For example, if you're doing image classification, you might replace the final fully connected layer with one that has the number of output classes matching your problem.\\n   * **Freeze Layers:** To prevent drastic changes to the pre-trained knowledge, consider freezing some of the earlier layers in the network. Only fine-tune the later layers that are more specific to your task.\\n\\n5. **Define the Loss Function and Optimizer:**\\n\\n   * **Loss Function:** Choose a loss function that measures the difference between your model's predictions and the actual target values. Common choices include:\\n      * **Cross-entropy:** For classification tasks\\n      * **Mean Squared Error (MSE):** For regression tasks\\n   * **Optimizer:** Select an optimization algorithm to update the model's weights during training. Popular optimizers include:\\n      * **Adam**\\n      * **SGD (Stochastic Gradient Descent)**\\n\\n6. **Train the Model:**\\n\\n   * Feed your training data into the model and use the optimizer to adjust the weights based on the calculated loss.\\n   * Monitor the validation set performance during training to prevent overfitting (where the model performs well on the training data but poorly on unseen data).\\n\\n7. **Evaluate the Model:**\\n\\n   * After training, evaluate the fine-tuned model on your test set to assess its overall performance.\\n\\n8. **Iterate and Improve:**\\n\\n   * Fine-tuning is often an iterative process. Experiment with different hyperparameters (learning rate, batch size, number of epochs), pre-trained models, and architectural modifications to further improve your model's performance.\\n\\n\\n**Key Points:**\\n\\n* **Start small:** Begin with a smaller subset of your dataset and a less complex model to get a feel for the process.\\n* **Patience:** Fine-tuning can take time, especially with large models. Be patient and allow the model to learn.\\n* **Experiment:** Don't be afraid to try different approaches and see what works best for your specific task and dataset.\\n\\nLet me know if you'd like a more in-depth explanation of any of these steps or have a particular fine-tuning scenario in mind.\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 862, 'prompt_tokens': 17, 'total_tokens': 879, 'completion_time': 1.567272727, 'prompt_time': 9.311e-05, 'queue_time': 0.013730098, 'total_time': 1.567365837}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-3ec30d97-5b3c-437f-9d7d-16685b638725-0', usage_metadata={'input_tokens': 17, 'output_tokens': 862, 'total_tokens': 879})}])\n",
            "content=\"Let's break down how to fine-tune a pre-trained neural network.\\n\\n**Why Fine-Tune?**\\n\\nFine-tuning is like taking a brilliant student (your pre-trained model) who has a strong foundation in general knowledge and specializing them in your specific subject.\\n\\n* **Efficiency:**  Training a neural network from scratch is computationally expensive and time-consuming. Fine-tuning leverages the knowledge already learned by a pre-trained model, significantly reducing training time and resources.\\n* **Improved Performance:** Fine-tuning often leads to better performance on your target task compared to training a model from scratch, especially if your dataset is relatively small.\\n\\n**Steps to Fine-Tune a Neural Network**\\n\\n1. **Choose a Pre-trained Model:**\\n\\n   * Select a model architecture that's relevant to your task.  Popular choices include:\\n     * **Image Classification:** ResNet, Inception, VGG\\n     * **Natural Language Processing (NLP):** BERT, GPT, RoBERTa\\n     * **Object Detection:** YOLO, Faster R-CNN\\n   * Consider the size and complexity of the model. Larger models have more parameters and require more resources.\\n\\n2. **Prepare Your Dataset:**\\n\\n   * **Clean and Format:** Ensure your data is clean, well-formatted, and in a structure compatible with your chosen model.\\n   * **Split:** Divide your dataset into training, validation, and test sets. The training set is used to fine-tune the model, the validation set helps monitor performance during training, and the test set is used for a final evaluation of the model's ability to generalize to unseen data.\\n\\n3. **Load the Pre-trained Model:**\\n\\n   * Use a deep learning library like TensorFlow or PyTorch to load the pre-trained weights into your chosen model architecture.\\n\\n4. **Adjust the Model (if needed):**\\n\\n   * **Add/Remove Layers:** You might need to add or remove layers to match your specific task. For example, if you're doing image classification, you might replace the final fully connected layer with one that has the number of output classes matching your problem.\\n   * **Freeze Layers:** To prevent drastic changes to the pre-trained knowledge, consider freezing some of the earlier layers in the network. Only fine-tune the later layers that are more specific to your task.\\n\\n5. **Define the Loss Function and Optimizer:**\\n\\n   * **Loss Function:** Choose a loss function that measures the difference between your model's predictions and the actual target values. Common choices include:\\n      * **Cross-entropy:** For classification tasks\\n      * **Mean Squared Error (MSE):** For regression tasks\\n   * **Optimizer:** Select an optimization algorithm to update the model's weights during training. Popular optimizers include:\\n      * **Adam**\\n      * **SGD (Stochastic Gradient Descent)**\\n\\n6. **Train the Model:**\\n\\n   * Feed your training data into the model and use the optimizer to adjust the weights based on the calculated loss.\\n   * Monitor the validation set performance during training to prevent overfitting (where the model performs well on the training data but poorly on unseen data).\\n\\n7. **Evaluate the Model:**\\n\\n   * After training, evaluate the fine-tuned model on your test set to assess its overall performance.\\n\\n8. **Iterate and Improve:**\\n\\n   * Fine-tuning is often an iterative process. Experiment with different hyperparameters (learning rate, batch size, number of epochs), pre-trained models, and architectural modifications to further improve your model's performance.\\n\\n\\n**Key Points:**\\n\\n* **Start small:** Begin with a smaller subset of your dataset and a less complex model to get a feel for the process.\\n* **Patience:** Fine-tuning can take time, especially with large models. Be patient and allow the model to learn.\\n* **Experiment:** Don't be afraid to try different approaches and see what works best for your specific task and dataset.\\n\\nLet me know if you'd like a more in-depth explanation of any of these steps or have a particular fine-tuning scenario in mind.\\n\" additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 862, 'prompt_tokens': 17, 'total_tokens': 879, 'completion_time': 1.567272727, 'prompt_time': 9.311e-05, 'queue_time': 0.013730098, 'total_time': 1.567365837}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None} id='run-3ec30d97-5b3c-437f-9d7d-16685b638725-0' usage_metadata={'input_tokens': 17, 'output_tokens': 862, 'total_tokens': 879}\n",
            "Assistant: Let's break down how to fine-tune a pre-trained neural network.\n",
            "\n",
            "**Why Fine-Tune?**\n",
            "\n",
            "Fine-tuning is like taking a brilliant student (your pre-trained model) who has a strong foundation in general knowledge and specializing them in your specific subject.\n",
            "\n",
            "* **Efficiency:**  Training a neural network from scratch is computationally expensive and time-consuming. Fine-tuning leverages the knowledge already learned by a pre-trained model, significantly reducing training time and resources.\n",
            "* **Improved Performance:** Fine-tuning often leads to better performance on your target task compared to training a model from scratch, especially if your dataset is relatively small.\n",
            "\n",
            "**Steps to Fine-Tune a Neural Network**\n",
            "\n",
            "1. **Choose a Pre-trained Model:**\n",
            "\n",
            "   * Select a model architecture that's relevant to your task.  Popular choices include:\n",
            "     * **Image Classification:** ResNet, Inception, VGG\n",
            "     * **Natural Language Processing (NLP):** BERT, GPT, RoBERTa\n",
            "     * **Object Detection:** YOLO, Faster R-CNN\n",
            "   * Consider the size and complexity of the model. Larger models have more parameters and require more resources.\n",
            "\n",
            "2. **Prepare Your Dataset:**\n",
            "\n",
            "   * **Clean and Format:** Ensure your data is clean, well-formatted, and in a structure compatible with your chosen model.\n",
            "   * **Split:** Divide your dataset into training, validation, and test sets. The training set is used to fine-tune the model, the validation set helps monitor performance during training, and the test set is used for a final evaluation of the model's ability to generalize to unseen data.\n",
            "\n",
            "3. **Load the Pre-trained Model:**\n",
            "\n",
            "   * Use a deep learning library like TensorFlow or PyTorch to load the pre-trained weights into your chosen model architecture.\n",
            "\n",
            "4. **Adjust the Model (if needed):**\n",
            "\n",
            "   * **Add/Remove Layers:** You might need to add or remove layers to match your specific task. For example, if you're doing image classification, you might replace the final fully connected layer with one that has the number of output classes matching your problem.\n",
            "   * **Freeze Layers:** To prevent drastic changes to the pre-trained knowledge, consider freezing some of the earlier layers in the network. Only fine-tune the later layers that are more specific to your task.\n",
            "\n",
            "5. **Define the Loss Function and Optimizer:**\n",
            "\n",
            "   * **Loss Function:** Choose a loss function that measures the difference between your model's predictions and the actual target values. Common choices include:\n",
            "      * **Cross-entropy:** For classification tasks\n",
            "      * **Mean Squared Error (MSE):** For regression tasks\n",
            "   * **Optimizer:** Select an optimization algorithm to update the model's weights during training. Popular optimizers include:\n",
            "      * **Adam**\n",
            "      * **SGD (Stochastic Gradient Descent)**\n",
            "\n",
            "6. **Train the Model:**\n",
            "\n",
            "   * Feed your training data into the model and use the optimizer to adjust the weights based on the calculated loss.\n",
            "   * Monitor the validation set performance during training to prevent overfitting (where the model performs well on the training data but poorly on unseen data).\n",
            "\n",
            "7. **Evaluate the Model:**\n",
            "\n",
            "   * After training, evaluate the fine-tuned model on your test set to assess its overall performance.\n",
            "\n",
            "8. **Iterate and Improve:**\n",
            "\n",
            "   * Fine-tuning is often an iterative process. Experiment with different hyperparameters (learning rate, batch size, number of epochs), pre-trained models, and architectural modifications to further improve your model's performance.\n",
            "\n",
            "\n",
            "**Key Points:**\n",
            "\n",
            "* **Start small:** Begin with a smaller subset of your dataset and a less complex model to get a feel for the process.\n",
            "* **Patience:** Fine-tuning can take time, especially with large models. Be patient and allow the model to learn.\n",
            "* **Experiment:** Don't be afraid to try different approaches and see what works best for your specific task and dataset.\n",
            "\n",
            "Let me know if you'd like a more in-depth explanation of any of these steps or have a particular fine-tuning scenario in mind.\n",
            "\n",
            "User: what is your max token limit ?\n",
            "dict_values([{'messages': AIMessage(content='I can process a context of up to **4096 tokens**. This means I can understand and respond to messages of that length.\\n\\nKeep in mind that this limit includes both your input and my response. So, if you have a very long message, my response might be shorter to stay within the limit.\\n', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 67, 'prompt_tokens': 16, 'total_tokens': 83, 'completion_time': 0.121818182, 'prompt_time': 9.1869e-05, 'queue_time': 0.01365013, 'total_time': 0.121910051}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-3fd4134c-af2a-42d1-89b1-77c79dc259c4-0', usage_metadata={'input_tokens': 16, 'output_tokens': 67, 'total_tokens': 83})}])\n",
            "content='I can process a context of up to **4096 tokens**. This means I can understand and respond to messages of that length.\\n\\nKeep in mind that this limit includes both your input and my response. So, if you have a very long message, my response might be shorter to stay within the limit.\\n' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 67, 'prompt_tokens': 16, 'total_tokens': 83, 'completion_time': 0.121818182, 'prompt_time': 9.1869e-05, 'queue_time': 0.01365013, 'total_time': 0.121910051}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None} id='run-3fd4134c-af2a-42d1-89b1-77c79dc259c4-0' usage_metadata={'input_tokens': 16, 'output_tokens': 67, 'total_tokens': 83}\n",
            "Assistant: I can process a context of up to **4096 tokens**. This means I can understand and respond to messages of that length.\n",
            "\n",
            "Keep in mind that this limit includes both your input and my response. So, if you have a very long message, my response might be shorter to stay within the limit.\n",
            "\n",
            "User: what is your model name ?\n",
            "dict_values([{'messages': AIMessage(content='My model name is Gemma.\\n', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 15, 'total_tokens': 24, 'completion_time': 0.016363636, 'prompt_time': 8.407e-05, 'queue_time': 0.013684149, 'total_time': 0.016447706}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-81d2bf4c-a7c6-46ef-8a0f-f5f9a6383aaf-0', usage_metadata={'input_tokens': 15, 'output_tokens': 9, 'total_tokens': 24})}])\n",
            "content='My model name is Gemma.\\n' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 15, 'total_tokens': 24, 'completion_time': 0.016363636, 'prompt_time': 8.407e-05, 'queue_time': 0.013684149, 'total_time': 0.016447706}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None} id='run-81d2bf4c-a7c6-46ef-8a0f-f5f9a6383aaf-0' usage_metadata={'input_tokens': 15, 'output_tokens': 9, 'total_tokens': 24}\n",
            "Assistant: My model name is Gemma.\n",
            "\n",
            "User: bye'\n",
            "dict_values([{'messages': AIMessage(content='Goodbye! Have a great day. 😊  \\n\\nLet me know if you need anything else.\\n', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 11, 'total_tokens': 33, 'completion_time': 0.04, 'prompt_time': 9.061e-05, 'queue_time': 0.013601319, 'total_time': 0.04009061}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-3a49bbd6-caed-4eae-96da-76fa600ecb23-0', usage_metadata={'input_tokens': 11, 'output_tokens': 22, 'total_tokens': 33})}])\n",
            "content='Goodbye! Have a great day. 😊  \\n\\nLet me know if you need anything else.\\n' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 11, 'total_tokens': 33, 'completion_time': 0.04, 'prompt_time': 9.061e-05, 'queue_time': 0.013601319, 'total_time': 0.04009061}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None} id='run-3a49bbd6-caed-4eae-96da-76fa600ecb23-0' usage_metadata={'input_tokens': 11, 'output_tokens': 22, 'total_tokens': 33}\n",
            "Assistant: Goodbye! Have a great day. 😊  \n",
            "\n",
            "Let me know if you need anything else.\n",
            "\n",
            "User: Bye\n",
            "dict_values([{'messages': AIMessage(content='Bye! 👋 \\n\\nHave a great day!\\n', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 10, 'total_tokens': 23, 'completion_time': 0.023636364, 'prompt_time': 5.5e-07, 'queue_time': 0.013666249, 'total_time': 0.023636914}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-cd52c574-4faf-47e3-9e5c-fcc6625ec7ec-0', usage_metadata={'input_tokens': 10, 'output_tokens': 13, 'total_tokens': 23})}])\n",
            "content='Bye! 👋 \\n\\nHave a great day!\\n' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 10, 'total_tokens': 23, 'completion_time': 0.023636364, 'prompt_time': 5.5e-07, 'queue_time': 0.013666249, 'total_time': 0.023636914}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None} id='run-cd52c574-4faf-47e3-9e5c-fcc6625ec7ec-0' usage_metadata={'input_tokens': 10, 'output_tokens': 13, 'total_tokens': 23}\n",
            "Assistant: Bye! 👋 \n",
            "\n",
            "Have a great day!\n",
            "\n",
            "User: Good bye\n",
            "dict_values([{'messages': AIMessage(content='Goodbye! \\n\\nIt was nice chatting with you. Have a great day. 😊\\n', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 20, 'prompt_tokens': 11, 'total_tokens': 31, 'completion_time': 0.036363636, 'prompt_time': 8.6739e-05, 'queue_time': 0.0133464, 'total_time': 0.036450375}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-e6569b70-fe0e-4363-871b-12e5e33455cf-0', usage_metadata={'input_tokens': 11, 'output_tokens': 20, 'total_tokens': 31})}])\n",
            "content='Goodbye! \\n\\nIt was nice chatting with you. Have a great day. 😊\\n' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 20, 'prompt_tokens': 11, 'total_tokens': 31, 'completion_time': 0.036363636, 'prompt_time': 8.6739e-05, 'queue_time': 0.0133464, 'total_time': 0.036450375}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None} id='run-e6569b70-fe0e-4363-871b-12e5e33455cf-0' usage_metadata={'input_tokens': 11, 'output_tokens': 20, 'total_tokens': 31}\n",
            "Assistant: Goodbye! \n",
            "\n",
            "It was nice chatting with you. Have a great day. 😊\n",
            "\n",
            "User: Good Bye\n",
            "dict_values([{'messages': AIMessage(content='Goodbye!  \\n\\nIt was nice chatting with you.  Let me know if you have any other questions in the future.\\n', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 28, 'prompt_tokens': 11, 'total_tokens': 39, 'completion_time': 0.050909091, 'prompt_time': 9.1319e-05, 'queue_time': 0.01324308, 'total_time': 0.05100041}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-a3d158c2-5c12-45a8-95b3-0ce69f905114-0', usage_metadata={'input_tokens': 11, 'output_tokens': 28, 'total_tokens': 39})}])\n",
            "content='Goodbye!  \\n\\nIt was nice chatting with you.  Let me know if you have any other questions in the future.\\n' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 28, 'prompt_tokens': 11, 'total_tokens': 39, 'completion_time': 0.050909091, 'prompt_time': 9.1319e-05, 'queue_time': 0.01324308, 'total_time': 0.05100041}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None} id='run-a3d158c2-5c12-45a8-95b3-0ce69f905114-0' usage_metadata={'input_tokens': 11, 'output_tokens': 28, 'total_tokens': 39}\n",
            "Assistant: Goodbye!  \n",
            "\n",
            "It was nice chatting with you.  Let me know if you have any other questions in the future.\n",
            "\n",
            "User: q\n",
            "Good Bye\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DdFOCt5dTdg-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6siJMlLSTdjY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QyINQIMBSv2K"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}